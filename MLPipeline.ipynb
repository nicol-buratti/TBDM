{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65958379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd36ff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "print(pyspark.__version__)  # VERSION MUST MATCH THE SPARK CONTAINER VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42a529f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = \"bolt://neo4j:password@neo4j:7687\"\n",
    "graph_name = \"PeopleKnowledge\"\n",
    "pipeline_name = f\"LinkPrediction-{randint(0, 10**9)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f720d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cb19395d3b76:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MLPipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5bf3102cd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"MLPipeline\")\n",
    "    .master(\"spark://spark:7077\")\n",
    "    .config(\"spark.jars.packages\", \"neo4j-contrib:neo4j-spark-connector:5.3.1-s_2.12\")\n",
    "    .config(\"neo4j.url\", NEO4J_URI)\n",
    "    .config(\"neo4j.authentication.basic.username\", \"neo4j\")\n",
    "    .config(\"neo4j.authentication.basic.password\", \"password\")\n",
    "    .config(\"neo4j.database\", \"neo4j\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd7c064",
   "metadata": {},
   "source": [
    "# Create the projection graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fac30f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+---------------+---------+-----------------+-------------+\n",
      "|nodeProjection                                                                            |relationshipProjection                                                                                               |graphName      |nodeCount|relationshipCount|projectMillis|\n",
      "+------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+---------------+---------+-----------------+-------------+\n",
      "|{Person -> {\"properties\":{},\"label\":\"Person\"}, Paper -> {\"properties\":{},\"label\":\"Paper\"}}|{AUTHOR -> {\"aggregation\":\"DEFAULT\",\"orientation\":\"UNDIRECTED\",\"type\":\"AUTHOR\",\"properties\":{},\"indexInverse\":false}}|PeopleKnowledge|16369    |49282            |44           |\n",
      "+------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+---------------+---------+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop Graph if exists\n",
    "(\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\n",
    "        \"query\",\n",
    "        f\"CALL gds.graph.drop('{graph_name}', false) YIELD graphName RETURN graphName\",\n",
    "    )\n",
    "    .option(\"partitions\", \"1\")\n",
    "    .load()\n",
    ")\n",
    "# Create graph\n",
    "(\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"gds\", \"gds.graph.project\")\n",
    "    .option(\"gds.graphName\", graph_name)\n",
    "    .option(\"gds.nodeProjection\", [\"Person\", \"Paper\"])\n",
    "    .option(\n",
    "        \"gds.relationshipProjection\",\n",
    "        '{\"AUTHOR\": {\"orientation\": \"UNDIRECTED\"}}',\n",
    "    )\n",
    "    .load()\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1f22836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vol_rel = (\n",
    "#     spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "#     .option(\"relationship\", \"EDITOR\")\n",
    "#     .option(\"relationship.source.labels\", \"Volume\")\n",
    "#     .option(\"relationship.target.labels\", \"Person\")\n",
    "#     .load()\n",
    "#     .select(\n",
    "#         col(\"`<source.id>`\").alias(\"source_id\"), col(\"`<target.id>`\").alias(\"person_id\")\n",
    "#     )\n",
    "# )\n",
    "# vol_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "218e5c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pap_rel = (\n",
    "#     spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "#     .option(\"relationship\", \"AUTHOR\")\n",
    "#     .option(\"relationship.source.labels\", \"Paper\")\n",
    "#     .option(\"relationship.target.labels\", \"Person\")\n",
    "#     .load()\n",
    "#     .select(\n",
    "#         col(\"`<source.id>`\").alias(\"source_id\"), col(\"`<target.id>`\").alias(\"person_id\")\n",
    "#     )\n",
    "# )\n",
    "# pap_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82a169d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # match connections between people that collaborated in papers and volumes\n",
    "# df = vol_rel.union(pap_rel)\n",
    "\n",
    "# df1 = df.alias(\"df1\")\n",
    "# df2 = df.alias(\"df2\")\n",
    "\n",
    "# df = df1.join(\n",
    "#     df2,\n",
    "#     (col(\"df1.source_id\") == col(\"df2.source_id\"))\n",
    "#     & (col(\"df1.person_id\") < col(\"df2.person_id\")),\n",
    "# ).select(col(\"df1.person_id\").alias(\"p1\"), col(\"df2.person_id\").alias(\"p2\"))\n",
    "\n",
    "\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1973c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "#     .option(\"gds\", \"gds.graph.project\")\n",
    "#     .option(\"gds.graphName\", graph_name)\n",
    "#     .option(\"gds.nodeProjection\", [\"Person\", \"Paper\"])\n",
    "#     .option(\n",
    "#         \"gds.relationshipProjection\",\n",
    "#         \"\"\"\n",
    "#         {\n",
    "#         \"AUTHOR\": {\"orientation\": \"UNDIRECTED\"},\n",
    "#         }\n",
    "#         \"\"\",\n",
    "#     )\n",
    "#     .load()\n",
    "#     .show(truncate=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7183ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "# .option(\"gds\", \"gds.graph.relationship.write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ffcb66",
   "metadata": {},
   "source": [
    "## Creating a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc6053b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+\n",
      "|pipelineName|pipelineType|creationTime|\n",
      "+------------+------------+------------+\n",
      "+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\n",
    "        \"query\",\n",
    "        f\"\"\"CALL gds.beta.pipeline.drop('{pipeline_name}', false)\n",
    "        YIELD pipelineName, pipelineType, creationTime\n",
    "        RETURN pipelineName, pipelineType, creationTime\"\"\",\n",
    "    )\n",
    "    .option(\"partitions\", \"1\")\n",
    "    .load()\n",
    "    .show(truncate=False)\n",
    ")\n",
    "try:\n",
    "    (\n",
    "        spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\n",
    "            \"query\",\n",
    "            f\"\"\"CALL gds.beta.pipeline.linkPrediction.create('{pipeline_name}')\n",
    "            YIELD name, nodePropertySteps, featureSteps, splitConfig, autoTuningConfig, parameterSpace\n",
    "            RETURN name, nodePropertySteps, featureSteps, splitConfig, autoTuningConfig, parameterSpace\"\"\",\n",
    "        )\n",
    "        .option(\"partitions\", \"1\")\n",
    "        .load()\n",
    "        .show(truncate=False)\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e4aa0",
   "metadata": {},
   "source": [
    "### Adding node properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea53bdd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o254.load.\n: org.neo4j.driver.exceptions.ClientException: Failed to invoke procedure `gds.beta.pipeline.linkPrediction.addNodeProperty`: Caused by: java.lang.IllegalArgumentException: The value of `mutateProperty` is expected to be unique, but embedding was already specified in the gds.fastRP.mutate procedure.\n\tat org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:111)\n\tat org.neo4j.driver.internal.InternalResult.blockingGet(InternalResult.java:107)\n\tat org.neo4j.driver.internal.InternalResult.list(InternalResult.java:88)\n\tat org.neo4j.spark.service.SchemaService.retrieveSchema(SchemaService.scala:111)\n\tat org.neo4j.spark.service.SchemaService.structForQuery(SchemaService.scala:233)\n\tat org.neo4j.spark.service.SchemaService.struct(SchemaService.scala:341)\n\tat org.neo4j.spark.DataSource.$anonfun$inferSchema$1(DataSource.scala:29)\n\tat org.neo4j.spark.util.Neo4jUtil$.callSchemaService(Neo4jUtil.scala:173)\n\tat org.neo4j.spark.DataSource.inferSchema(DataSource.scala:29)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:90)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:140)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:210)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat jdk.internal.reflect.GeneratedMethodAccessor41.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.neo4j.driver.internal.util.ErrorUtil$InternalExceptionCause\n\t\tat org.neo4j.driver.internal.util.ErrorUtil.newNeo4jError(ErrorUtil.java:76)\n\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageDispatcher.handleFailureMessage(InboundMessageDispatcher.java:107)\n\t\tat org.neo4j.driver.internal.messaging.common.CommonMessageReader.unpackFailureMessage(CommonMessageReader.java:75)\n\t\tat org.neo4j.driver.internal.messaging.common.CommonMessageReader.read(CommonMessageReader.java:53)\n\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageHandler.channelRead0(InboundMessageHandler.java:81)\n\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageHandler.channelRead0(InboundMessageHandler.java:37)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)\n\t\tat org.neo4j.driver.internal.async.inbound.MessageDecoder.channelRead(MessageDecoder.java:42)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1407)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:918)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 20\u001b[0m\n\u001b[1;32m      1\u001b[0m (\n\u001b[1;32m      2\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.neo4j.spark.DataSource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m    CALL gds.beta.pipeline.linkPrediction.addNodeProperty(\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m  \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpipeline_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m  \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfastRP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43m  \u001b[39;49m\u001b[38;5;130;43;01m{{\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m    mutateProperty: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43m    embeddingDimension: 256,\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43m    randomSeed: 42\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;43m  \u001b[39;49m\u001b[38;5;130;43;01m}}\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43mYIELD name, nodePropertySteps, featureSteps, splitConfig, autoTuningConfig, parameterSpace\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43mRETURN name, nodePropertySteps, featureSteps, splitConfig, autoTuningConfig, parameterSpace\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpartitions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:314\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o254.load.\n: org.neo4j.driver.exceptions.ClientException: Failed to invoke procedure `gds.beta.pipeline.linkPrediction.addNodeProperty`: Caused by: java.lang.IllegalArgumentException: The value of `mutateProperty` is expected to be unique, but embedding was already specified in the gds.fastRP.mutate procedure.\n\tat org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:111)\n\tat org.neo4j.driver.internal.InternalResult.blockingGet(InternalResult.java:107)\n\tat org.neo4j.driver.internal.InternalResult.list(InternalResult.java:88)\n\tat org.neo4j.spark.service.SchemaService.retrieveSchema(SchemaService.scala:111)\n\tat org.neo4j.spark.service.SchemaService.structForQuery(SchemaService.scala:233)\n\tat org.neo4j.spark.service.SchemaService.struct(SchemaService.scala:341)\n\tat org.neo4j.spark.DataSource.$anonfun$inferSchema$1(DataSource.scala:29)\n\tat org.neo4j.spark.util.Neo4jUtil$.callSchemaService(Neo4jUtil.scala:173)\n\tat org.neo4j.spark.DataSource.inferSchema(DataSource.scala:29)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:90)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:140)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:210)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat jdk.internal.reflect.GeneratedMethodAccessor41.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.neo4j.driver.internal.util.ErrorUtil$InternalExceptionCause\n\t\tat org.neo4j.driver.internal.util.ErrorUtil.newNeo4jError(ErrorUtil.java:76)\n\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageDispatcher.handleFailureMessage(InboundMessageDispatcher.java:107)\n\t\tat org.neo4j.driver.internal.messaging.common.CommonMessageReader.unpackFailureMessage(CommonMessageReader.java:75)\n\t\tat org.neo4j.driver.internal.messaging.common.CommonMessageReader.read(CommonMessageReader.java:53)\n\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageHandler.channelRead0(InboundMessageHandler.java:81)\n\t\tat org.neo4j.driver.internal.async.inbound.InboundMessageHandler.channelRead0(InboundMessageHandler.java:37)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)\n\t\tat org.neo4j.driver.internal.async.inbound.MessageDecoder.channelRead(MessageDecoder.java:42)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1407)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:918)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\t\tat org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\n",
    "        \"query\",\n",
    "        f\"\"\"\n",
    "    CALL gds.beta.pipeline.linkPrediction.addNodeProperty(\n",
    "  '{pipeline_name}',\n",
    "  'fastRP',\n",
    "  {{\n",
    "    mutateProperty: 'embedding',\n",
    "    embeddingDimension: 256,\n",
    "    randomSeed: 42\n",
    "  }}\n",
    ")\n",
    "YIELD name, nodePropertySteps, featureSteps, splitConfig, autoTuningConfig, parameterSpace\n",
    "RETURN name, nodePropertySteps, featureSteps, splitConfig, autoTuningConfig, parameterSpace\n",
    "\"\"\",\n",
    "    )\n",
    "    .option(\"partitions\", \"1\")\n",
    "    .load()\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8d029df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\n",
    "        \"query\",\n",
    "        f\"\"\"\n",
    "CALL gds.beta.pipeline.linkPrediction.addNodeProperty(\n",
    "  pipelineName: \"{pipeline_name}\",\n",
    "  procedureName: 'degree',\n",
    "  configuration: {{\n",
    "    mutateProperty: 'degree'\n",
    "  }}\n",
    ")\n",
    "\"\"\",\n",
    "    )\n",
    "    .option(\"partitions\", \"1\")\n",
    "    # .option(\"gds\", \"gds.beta.pipeline.linkPrediction.addNodeProperty\")\n",
    "    # .option(\"gds.pipelineName\", pipeline_name)\n",
    "    # .option(\"gds.procedureName\", \"degree\")\n",
    "    # .option(\"gds.procedureConfiguration.mutateProperty\", \"degree\")\n",
    "    .load()\n",
    "    # .show()\n",
    ")\n",
    "(\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\n",
    "        \"query\",\n",
    "        f\"\"\"\n",
    "        CALL gds.beta.pipeline.linkPrediction.addNodeProperty(\n",
    "  pipelineName: \"{pipeline_name}\",\n",
    "  procedureName: 'alpha.scaleProperties',\n",
    "  configuration: {{\n",
    "    nodeProperties: ['degree'],\n",
    "    mutateProperty: 'scaledDegree',\n",
    "    scaler: 'MinMax'\n",
    "  }}\n",
    ")\n",
    "\"\"\",\n",
    "    )\n",
    "    .option(\"partitions\", \"1\")\n",
    "    # .option(\"gds\", \"gds.beta.pipeline.linkPrediction.addNodeProperty\")\n",
    "    # .option(\"gds.pipelineName\", pipeline_name)\n",
    "    # .option(\"gds.procedureName\", \"alpha.scaleProperties\")\n",
    "    # .option(\"gds.procedureConfiguration.nodeProperties\", [\"degree\"])\n",
    "    # .option(\"gds.procedureConfiguration.mutateProperty\", \"scaledDegree\")\n",
    "    # .option(\"gds.procedureConfiguration.scaler\", \"MinMax\")\n",
    "    .load()\n",
    "    # .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8d21f",
   "metadata": {},
   "source": [
    "### Adding link properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c4d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\n",
    "        \"query\",\n",
    "        f\"\"\"\n",
    "CALL gds.beta.pipeline.linkPrediction.addFeature(\n",
    "  pipelineName: \"{pipeline_name}\",\n",
    "  featureType: 'hadamard',\n",
    "  configuration: {{\n",
    "    nodeProperties: ['scaledDegree']\n",
    "  }}\n",
    ")\n",
    "\"\"\",\n",
    "    )\n",
    "    .option(\"partitions\", \"1\")\n",
    "    # .option(\"gds\", \"gds.beta.pipeline.linkPrediction.addFeature\")\n",
    "    # .option(\"gds.pipelineName\", pipeline_name)\n",
    "    # .option(\"gds.featureType\", \"hadamard\")\n",
    "    # .option(\"gds.configuration.nodeProperties\", [\"scaledDegree\"])\n",
    "    .load()\n",
    "    # .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a21f0",
   "metadata": {},
   "source": [
    "### Configuring the relationship splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68dbd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\n",
    "        \"query\",\n",
    "        f\"\"\"CALL gds.beta.pipeline.linkPrediction.train(\n",
    "  pipelineName: \"{pipeline_name}\",\n",
    "  graphName: \"{graph_name}\",\n",
    "  configuration: {{\n",
    "    testFraction: 0.1,\n",
    "    trainFraction: 0.1,\n",
    "    validationFolds: 3\n",
    "  }}\n",
    ")\n",
    "            \"\"\",\n",
    "    )\n",
    "    .option(\"partitions\", \"1\")\n",
    "    # .option(\"gds\", \"gds.beta.pipeline.linkPrediction.addFeature\")\n",
    "    # .option(\"gds.pipelineName\", pipeline_name)\n",
    "    # .option(\"gds.configuration.testFraction\", 0.1)\n",
    "    # .option(\"gds.configuration.trainFraction\", 0.1)\n",
    "    # .option(\"gds.configuration.validationFolds\", 3)\n",
    "    .load()\n",
    "    # .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00250bbb",
   "metadata": {},
   "source": [
    "### Adding model candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b5219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\n",
    "        \"query\",\n",
    "        f\"\"\"CALL gds.beta.pipeline.linkPrediction.addLogisticRegression(\n",
    "  pipelineName: \"{pipeline_name}\"\n",
    ")\n",
    "            \"\"\",\n",
    "    )\n",
    "    .option(\"partitions\", \"1\")\n",
    "    # .option(\"gds\", \"gds.beta.pipeline.linkPrediction.addLogisticRegression\")\n",
    "    # .option(\"gds.pipelineName\", pipeline_name)\n",
    "    .load()\n",
    "    # .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09556bfe",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113d74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[requiredMemory: string, treeView: string, mapView: map<string,string>, bytesMin: bigint, bytesMax: bigint, nodeCount: bigint, relationshipCount: bigint, heapPercentageMin: double, heapPercentageMax: double]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimate the training cost\n",
    "(\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"gds\", \"gds.beta.pipeline.linkPrediction.train.estimate\")\n",
    "    .option(\"gds.graphName\", graph_name)\n",
    "    .option(\"gds.configuration.modelName\", \"collaborationPrediction\")\n",
    "    .option(\"gds.configuration.pipeline\", pipeline_name)\n",
    "    .load()\n",
    "    # .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9aff88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[modelSelectionStats: map<string,string>, trainMillis: bigint, modelInfo: map<string,string>, configuration: map<string,string>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"gds\", \"gds.beta.pipeline.linkPrediction.train\")\n",
    "    .option(\"gds.graphName\", graph_name)\n",
    "    .option(\"gds.configuration.modelName\", \"collaborationPrediction\")\n",
    "    .option(\"gds.configuration.pipeline\", pipeline_name)\n",
    "    .option(\"gds.configuration.metrics\", [\"AUCPR\"])\n",
    "    .option(\"gds.configuration.randomSeed\", 42)\n",
    "    .load()\n",
    "    # .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67314e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[relationshipsWritten: bigint, probabilityDistribution: map<string,string>, samplingStats: map<string,string>, mutateMillis: bigint, postProcessingMillis: bigint, preProcessingMillis: bigint, computeMillis: bigint, configuration: map<string,string>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"gds\", \"gds.beta.pipeline.linkPrediction.predict.mutate\")\n",
    "    .option(\"gds.graphName\", graph_name)\n",
    "    .option(\"gds.configuration.modelName\", \"collaborationPrediction\")\n",
    "    .option(\"gds.configuration.mutateRelationshipType\", \"AUTHOR_APPROX_PREDICTED\")\n",
    "    .option(\"gds.configuration.topN\", 40)\n",
    "    .option(\"gds.configuration.threshold\", 0.45)\n",
    "    .load()\n",
    "    # .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b90e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[node1: bigint, node2: bigint, probability: double]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"gds\", \"gds.beta.pipeline.linkPrediction.predict.stream\")\n",
    "    .option(\"gds.graphName\", graph_name)\n",
    "    .option(\"gds.configuration.modelName\", \"collaborationPrediction\")\n",
    "    .option(\"gds.configuration.topN\", 10)\n",
    "    .load()\n",
    "    # .toPandas()\n",
    "    # .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785867c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
