{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, regexp_extract\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = \"bolt://neo4j:password@neo4j:7687\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__) # VERSION MUST MATCH THE SPARK CONTAINER VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://37537350d23c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>JsonToNeo4jInjection</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc380139990>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"JsonToNeo4jInjection\")\n",
    "    .master(\"spark://spark:7077\")\n",
    "    .config(\"spark.jars.packages\", \"neo4j-contrib:neo4j-spark-connector:5.3.1-s_2.12\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"volnr\", StringType()),\n",
    "    StructField(\"pubyear\", StringType()),\n",
    "    StructField(\"volacronym\", StringType()),\n",
    "    StructField(\"voltitle\", StringType()),\n",
    "    StructField(\"fulltitle\", StringType()),\n",
    "    StructField(\"loctime\", StringType()),\n",
    "    StructField(\"voleditors\", ArrayType(StringType())),\n",
    "    StructField(\"papers\", ArrayType(StructType([\n",
    "        StructField(\"authors\", ArrayType(StringType())),\n",
    "        StructField(\"keywords\", ArrayType(StringType())),\n",
    "        StructField(\"url\", StringType()),\n",
    "        StructField(\"title\", StringType()),\n",
    "        StructField(\"pages\", StringType()),\n",
    "        StructField(\"abstract\", StringType()),\n",
    "    ])))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               title|   volnr|pubyear|    volacronym|            voltitle|           fulltitle|             loctime|          voleditors|              papers|\n",
      "+--------------------+--------+-------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|CEUR-WS.org/Vol-3...|Vol-3497|   2023|  CLEF-WN 2023|CLEF 2023 Working...|Working Notes of ...|Thessaloniki, Gre...|[Mohammad Alianne...|[{[], [], https:/...|\n",
      "|CEUR-WS.org/Vol-3...|Vol-3180|   2022|     CLEF 2022|CLEF 2022 Working...|Proceedings of th...|                NULL|[Guglielmo Faggio...|[{[Guglielmo Fagg...|\n",
      "|CEUR-WS.org/Vol-2...|Vol-2019|   2017|MODELS-SE 2017|MODELS 2017 Satel...|Proceedings of MO...|Austin, TX, USA, ...|[Loli Burgueño, J...|[{[Michael Shin, ...|\n",
      "|CEUR-WS.org/Vol-2...|Vol-2936|   2021|  CLEF-WN 2021|CLEF 2021 Working...|Proceedings of th...|                NULL|[Guglielmo Faggio...|[{[Guglielmo Fagg...|\n",
      "|CEUR-WS.org/Vol-3...|Vol-3486|   2023|  Ital-IA 2023|Ital-IA 2023 Them...|Proceedings of th...|Pisa, Italy, May ...|[Fabrizio Falchi,...|[{[Lorenzo De Don...|\n",
      "|CEUR-WS.org/Vol-3...|Vol-3293|   2022|   HAICTA 2022|Information and C...|Proceedings of th...|Athens, Greece, S...|[Alexandros Theod...|[{[Maria Spilioti...|\n",
      "|CEUR-WS.org/Vol-1...|Vol-1174|   2008|     CLEF 2008|CLEF2008 Working ...|Working Notes for...|Aarhus, Denmark, ...|[Carol Peters, Ni...|[{[], [], https:/...|\n",
      "|CEUR-WS.org/Vol-1...|Vol-1175|   2009|     CLEF 2009|CLEF2009 Working ...|Working Notes for...|Corfù, Greece, Se...|[Carol Peters, Ni...|[{[], [], https:/...|\n",
      "|CEUR-WS.org/Vol-1...|Vol-1178|   2012|     CLEF 2012|CLEF2012 Working ...|Working Notes for...|Rome, Italy, Sept...|[Pamela Forner, J...|[{[], [], https:/...|\n",
      "|CEUR-WS.org/Vol-1...|Vol-1429|   2012|    CURAC 2012|CURAC Annual Meeting|Proceedings of th...|Düsseldorf, Germa...|[Bernhard Preim, ...|[{[], [], https:/...|\n",
      "|CEUR-WS.org/Vol-2...|Vol-2380|   2019|     CLEF 2019|CLEF 2019 Working...|Working Notes of ...|Lugano, Switzerla...|[Linda Cappellato...|[{[Nicola Ferro, ...|\n",
      "|CEUR-WS.org/Vol-3...|Vol-3027|   2021|GraphiCon 2021|Computer Graphics...|Proceedings of th...|Nizhny Novgorod, ...|[Vladimir Galakti...|[{[Vladimir Frolo...|\n",
      "|CEUR-WS.org/Vol-3...|Vol-3171|   2022|   COLINS 2022|Computational Lin...|Proceedings of th...|Gliwice, Poland, ...|[Vasyl Lytvyn, Na...|[{[Marek Kuźniak]...|\n",
      "|CEUR-WS.org/Vol-3...|Vol-3159|   2021|  FIRE-WN 2021|FIRE 2021 Working...|Working Notes of ...|Gandhinagar, Indi...|[Parth Mehta, Tho...|[{[Parth Mehta, T...|\n",
      "|CEUR-WS.org/Vol-3...|Vol-3878|   2024|  CLiC-it 2024|Italian Conferenc...|Proceedings of th...|Pisa, Italy, Dece...|[Felice Dell'Orle...|[{[Aurora Alagni,...|\n",
      "|CEUR-WS.org/Vol-2...|Vol-2744|   2020|GraphiCon 2020|Computer Graphics...|Proceedings of th...|Saint Petersburg,...|[Sergei Bykovskii...|[{[Mattias Mende,...|\n",
      "|CEUR-WS.org/Vol-1...|Vol-1172|   2006|     CLEF 2006|CLEF2006 Working ...|Working Notes for...|Alicante, Spain, ...|[Alessandro Nardi...|[{[], [], https:/...|\n",
      "|CEUR-WS.org/Vol-1...|Vol-1173|   2007|     CLEF 2007|CLEF2007 Working ...|Working Notes for...|Budapest, Hungary...|[Alessandro Nardi...|[{[], [], https:/...|\n",
      "|CEUR-WS.org/Vol-2...|Vol-2940|   2021|   ITASEC 2021|Italian Conferenc...|Proceedings of th...|All Digital Event...|[Alessandro Arman...|[{[Didier Danet, ...|\n",
      "|CEUR-WS.org/Vol-2...|Vol-2125|   2018|     CLEF 2018|CLEF 2018 Working...|Working Notes of ...|Avignon, France, ...|[Linda Cappellato...|[{[Nicola Ferro, ...|\n",
      "+--------------------+--------+-------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to directory containing multiple JSON files\n",
    "from pathlib import Path\n",
    "\n",
    "json_dir = Path(\"../data/Volumes\").__str__()\n",
    "\n",
    "# Read all JSON files in the directory\n",
    "n = 200\n",
    "df = spark.read.schema(schema).option(\"multiline\", \"true\").json(json_dir).limit(n)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   volnr|\n",
      "+--------+\n",
      "|Vol-3497|\n",
      "|Vol-3180|\n",
      "|Vol-2019|\n",
      "|Vol-2936|\n",
      "|Vol-3486|\n",
      "|Vol-3293|\n",
      "|Vol-1174|\n",
      "|Vol-1175|\n",
      "|Vol-1178|\n",
      "|Vol-1429|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"volnr\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol = (\n",
    "    spark.read.option(\"url\", NEO4J_URI)\n",
    "    .option(\"authentication.type\", \"basic\")\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\n",
    "    .option(\"authentication.basic.password\", \"password\")\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"labels\", \":Volume\")\n",
    "    .load()\n",
    ")\n",
    "vol.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# if vol.count() > 0:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     df = df.join(vol, df[\"volnr\"] == vol[\"volnr\"], \"left_anti\")\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if vol.count() > 0:\n",
    "#     df = df.join(vol, df[\"volnr\"] == vol[\"volnr\"], \"left_anti\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create volume relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_param = [\n",
    "    \"volnr\",\n",
    "    \"title\",\n",
    "    \"pubyear\",\n",
    "    \"volacronym\",\n",
    "    \"voltitle\",\n",
    "    \"fulltitle\",\n",
    "    \"loctime\",\n",
    "]\n",
    "paper_param = [\"url\", \"abstract\", \"title\", \"pages\"]\n",
    "\n",
    "person_param = \"name\"\n",
    "keyword_param = \"name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume -> Paper (CONTAINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_papers = df.withColumn(\"paper\", explode(\"papers\")).select(\n",
    "    \"title\",\n",
    "    \"volnr\",\n",
    "    \"pubyear\",\n",
    "    \"volacronym\",\n",
    "    \"voltitle\",\n",
    "    \"fulltitle\",\n",
    "    \"loctime\",\n",
    "    col(\"paper.url\").alias(\"url\"),\n",
    "    col(\"paper.title\").alias(\"paper_title\"),\n",
    "    col(\"paper.pages\").alias(\"pages\"),\n",
    "    col(\"paper.abstract\").alias(\"abstract\"),\n",
    ")\n",
    "\n",
    "p_param = \",\".join(paper_param).replace(\"title\", \"paper_title:title\", 1)\n",
    "\n",
    "(\n",
    "    volume_papers.write\n",
    "    # Overwrite relationships\n",
    "    .mode(\"Overwrite\")\n",
    "    .option(\"url\", NEO4J_URI)\n",
    "    .option(\"authentication.type\", \"basic\")\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\n",
    "    .option(\"authentication.basic.password\", \"password\")\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    # Assign a type to the relationships\n",
    "    .option(\"relationship\", \"CONTAINS\")\n",
    "    # Use `keys` strategy\n",
    "    .option(\"relationship.save.strategy\", \"keys\")\n",
    "    # Overwrite source nodes and assign them a label\n",
    "    .option(\"relationship.source.save.mode\", \"Overwrite\")\n",
    "    .option(\"relationship.source.labels\", \":Volume\")\n",
    "    # Map the DataFrame columns to node properties\n",
    "    .option(\"relationship.source.node.properties\", \",\".join(volume_param))\n",
    "    # Node keys are mandatory for overwrite save mode\n",
    "    .option(\"relationship.source.node.keys\", \"volnr\")\n",
    "    # Overwrite target nodes and assign them a label\n",
    "    .option(\"relationship.target.save.mode\", \"Overwrite\")\n",
    "    .option(\"relationship.target.labels\", \":Paper\")\n",
    "    # Map the DataFrame columns to node properties\n",
    "    .option(\"relationship.target.node.properties\", p_param)\n",
    "    # Node keys are mandatory for overwrite save mode\n",
    "    .option(\"relationship.target.node.keys\", \"url\")\n",
    "    # Map the DataFrame columns to relationship properties\n",
    "    .option(\"relationship.properties\", \"\")\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_papers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume -> Person (EDITOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_editor = (\n",
    "    df.withColumn(\"voleditorname\", explode(\"voleditors\"))\n",
    "    .drop(\"voleditors\", \"papers\")\n",
    "    .dropna(subset=[\"voleditorname\"])\n",
    ")\n",
    "\n",
    "(\n",
    "    volume_editor.write\n",
    "    # Overwrite relationships\n",
    "    .mode(\"Overwrite\")\n",
    "    .option(\"url\", NEO4J_URI)\n",
    "    .option(\"authentication.type\", \"basic\")\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\n",
    "    .option(\"authentication.basic.password\", \"password\")\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    # Assign a type to the relationships\n",
    "    .option(\"relationship\", \"EDITOR\")\n",
    "    # Use `keys` strategy\n",
    "    .option(\"relationship.save.strategy\", \"keys\")\n",
    "    # Overwrite source nodes and assign them a label\n",
    "    .option(\"relationship.source.save.mode\", \"Overwrite\")\n",
    "    .option(\"relationship.source.labels\", \":Volume\")\n",
    "    # Map the DataFrame columns to node properties\n",
    "    .option(\"relationship.source.node.properties\", \",\".join(volume_param))\n",
    "    # Node keys are mandatory for overwrite save mode\n",
    "    .option(\"relationship.source.node.keys\", \"volnr\")\n",
    "    # Overwrite target nodes and assign them a label\n",
    "    .option(\"relationship.target.save.mode\", \"Overwrite\")\n",
    "    .option(\"relationship.target.labels\", \":Person\")\n",
    "    # Map the DataFrame columns to node properties\n",
    "    .option(\"relationship.target.node.properties\", \"voleditorname:name\")\n",
    "    # Node keys are mandatory for overwrite save mode\n",
    "    .option(\"relationship.target.node.keys\", \"voleditorname:name\")\n",
    "    # Map the DataFrame columns to relationship properties\n",
    "    .option(\"relationship.properties\", \"\")\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create papers relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = (df.withColumn(\"paper\", explode(\"papers\"))\n",
    "            .select(\n",
    "                col(\"paper.authors\").alias(\"authors\"),\n",
    "                col(\"paper.keywords\").alias(\"keywords\"),\n",
    "                col(\"paper.url\").alias(\"url\"),\n",
    "                col(\"paper.title\").alias(\"title\"),\n",
    "                col(\"paper.pages\").alias(\"pages\"),\n",
    "                col(\"paper.abstract\").alias(\"abstract\")\n",
    "            )\n",
    ")\n",
    "papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper -> Person (AUTHOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_authors = (\n",
    "    papers.withColumn(\"authorname\", explode(\"authors\"))\n",
    "    .drop(\"authors\", \"keywords\")\n",
    "    .dropna(subset=[\"authorname\"])\n",
    ")\n",
    "\n",
    "(\n",
    "    papers_authors.write\n",
    "    # Overwrite relationships\n",
    "    .mode(\"Overwrite\")\n",
    "    .option(\"url\", NEO4J_URI)\n",
    "    .option(\"authentication.type\", \"basic\")\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\n",
    "    .option(\"authentication.basic.password\", \"password\")\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    # Assign a type to the relationships\n",
    "    .option(\"relationship\", \"AUTHOR\")\n",
    "    # Use `keys` strategy\n",
    "    .option(\"relationship.save.strategy\", \"keys\")\n",
    "    # Overwrite source nodes and assign them a label\n",
    "    .option(\"relationship.source.save.mode\", \"Overwrite\")\n",
    "    .option(\"relationship.source.labels\", \":Paper\")\n",
    "    # Map the DataFrame columns to node properties\n",
    "    .option(\"relationship.source.node.properties\", \",\".join(paper_param))\n",
    "    # Node keys are mandatory for overwrite save mode\n",
    "    .option(\"relationship.source.node.keys\", \"url\")\n",
    "    # Overwrite target nodes and assign them a label\n",
    "    .option(\"relationship.target.save.mode\", \"Overwrite\")\n",
    "    .option(\"relationship.target.labels\", \":Person\")\n",
    "    # Map the DataFrame columns to node properties\n",
    "    .option(\"relationship.target.node.properties\", \"authorname:name\")\n",
    "    # Node keys are mandatory for overwrite save mode\n",
    "    .option(\"relationship.target.node.keys\", \"authorname:name\")\n",
    "    # Map the DataFrame columns to relationship properties\n",
    "    .option(\"relationship.properties\", \"\")\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper -> Keyword (KEYWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_authors = (\n",
    "    papers.withColumn(\"keyword\", explode(\"keywords\"))\n",
    "    .drop(\"authors\", \"keywords\")\n",
    "    .dropna(subset=[\"keyword\"])\n",
    ")\n",
    "\n",
    "(\n",
    "    papers_authors.write\n",
    "    # Overwrite relationships\n",
    "    .mode(\"Overwrite\")\n",
    "    .option(\"url\", NEO4J_URI)\n",
    "    .option(\"authentication.type\", \"basic\")\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\n",
    "    .option(\"authentication.basic.password\", \"password\")\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    # Assign a type to the relationships\n",
    "    .option(\"relationship\", \"KEYWORD\")\n",
    "    # Use `keys` strategy\n",
    "    .option(\"relationship.save.strategy\", \"keys\")\n",
    "    # Overwrite source nodes and assign them a label\n",
    "    .option(\"relationship.source.save.mode\", \"Overwrite\")\n",
    "    .option(\"relationship.source.labels\", \":Paper\")\n",
    "    # Map the DataFrame columns to node properties\n",
    "    .option(\"relationship.source.node.properties\", \",\".join(paper_param))\n",
    "    # Node keys are mandatory for overwrite save mode\n",
    "    .option(\"relationship.source.node.keys\", \"url\")\n",
    "    # Overwrite target nodes and assign them a label\n",
    "    .option(\"relationship.target.save.mode\", \"Overwrite\")\n",
    "    .option(\"relationship.target.labels\", \":Keyword\")\n",
    "    # Map the DataFrame columns to node properties\n",
    "    .option(\"relationship.target.node.properties\", \"keyword:name\")\n",
    "    # Node keys are mandatory for overwrite save mode\n",
    "    .option(\"relationship.target.node.keys\", \"keyword:name\")\n",
    "    # Map the DataFrame columns to relationship properties\n",
    "    .option(\"relationship.properties\", \"\")\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
